{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Neural Networks\n",
    "Lab Author: Calvin Josenhans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "The aim of this lab is to hopefully provide you with a better understanding of the mechanisms at the heart of machine learning and artificial intelligence. We'll build our own learning model from scratch, and use it to ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "You're probably already familiar with images like the one below.\n",
    "\n",
    "<img src=\".\\images\\Neural_network_explain.png\" alt=\"A Depiction of an Artificial Neural Network\" width=\"500\"/> \n",
    "\n",
    "\n",
    "This depicts an artificial neural network, which is one of many models that can be used in a machine learning system (wikipedia). It attempts to model the way that the human brain works, with connections between individual neurons that transmit signals based on a set of trained weights and biases.\n",
    "\n",
    "## More Neural network explanation\n",
    "If we zoom in on a single neuron in the network, it looks something like this:\n",
    "\n",
    "<img src=\".\\images\\neuron.png\" alt=\"A Single Neuron in a neural network\" width=\"500\">\n",
    "\n",
    "You can see that there are a set of inputs that correspond to the outputs of the previous layer, as well an output, which will get sent to all of the neurons in the next layer. You may also notice the values $w_1, ..., w_n$, which are called the weights of the neuron. Every input gets multiplied by that weight before getting summed with the other inputs, which can be thought of representing how 'important' that input is to the neuron.\n",
    "\n",
    "The function $f$ is known as the neuron's activation function, which converts the sum of all of the inputs to a value in the range $[0, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Own Neural Network\n",
    "Let's say that we want to create a simple neural network that can tell us what color a flower will be in a field.\n",
    "\n",
    "<img src=\".\\images\\Meadow-with-lots-of-yellow-flowers-scaled.jpg\" alt=\"A Depiction of an Artificial Neural Network\" width=\"500\"/> \n",
    "\n",
    "It turns out that we can solve this problem with the simplest possible neural network, consisting of a single neuron called a **perceptron**.\n",
    "\n",
    "## Getting Started with Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, sample\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_inputs):\n",
    "        # Giving the perceptron its learning constant\n",
    "        # This is a variable that controls how fast the perceptron learns\n",
    "        self.c = 0.2\n",
    "        self.weights = list()\n",
    "        for i in range(num_inputs):\n",
    "            self.weights.append(random() * 2 - 1)\n",
    "\n",
    "    # A very simple function that decices whether the perceptron outputs a signal or not\n",
    "    # Returns 1 if the number is positive, and -1 otherwise\n",
    "    def activate(self, n):\n",
    "        if n < 0: return -1\n",
    "        return 1\n",
    "\n",
    "    # Runs the inputs through the perceptron\n",
    "    def feedforward(self, inputs):\n",
    "        sum = 0\n",
    "        # Multiply every input by the corresponding weight\n",
    "        for i, input in enumerate(inputs):\n",
    "            sum += input * self.weights[i]\n",
    "\n",
    "        return self.activate(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Our Perceptron\n",
    "As our perceptron is, it can already make predictions about what color flowers we will see. Our perceptron will have 3 inputs, the x and y coordinates of the flower, and a bias term.\n",
    "\n",
    "The bias term may seem strange at first, but it begines to make sense if we think about what happens for the point (0, 0). No matter what the weights of the perceptron are, it will always output 0, which is not very useful. By adding the bias term we are able to account for this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting Function that We'll Use Later\n",
    "def plot_results(inputs, outputs):\n",
    "    xs = [x for (x, y, b) in inputs]\n",
    "    ys = [y for (x, y, b) in inputs]\n",
    "\n",
    "    plt.scatter(xs, ys, c=outputs, cmap='viridis')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a perceptron with 3 inputs, an x, y and bias.\n",
    "perceptron = Perceptron(3)\n",
    "\n",
    "# There is a flower at every coordinate point\n",
    "flower_locations = []\n",
    "for i in range(-25, 25):\n",
    "    for j in range(-25, 25):\n",
    "        flower_locations.append([i, j, 1])\n",
    "\n",
    "\n",
    "# Keeping track of the predictions that the perceptron makes\n",
    "flower_colors = []\n",
    "for loc in flower_locations:\n",
    "    prediction = perceptron.feedforward(loc)\n",
    "    flower_colors.append(prediction)\n",
    "\n",
    "plot_results(flower_locations, flower_colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Graph\n",
    "Notice the linear boundary between the two colors of flowers. This is because our network only has 1 neuron (so technically it isn't a network at all!). Increasing the size of our network allows for handling more complex data as we'll see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Right now, our perceptron is simply guessing at a distribution of flowers. The real power comes with the ability to train the perceptron based on just a few datapoints, to learn the pattern of data distribution.\n",
    "\n",
    "## A training algorithm\n",
    "1. Provide the perceptron with inputs where there is a known answer. \n",
    "2. Ask the perceptron to guess an answer for those inputs.\n",
    "3. Compute the error - i.e. how close to the correct answer was the perceptron.\n",
    "4. Adjust the weights based on the error.\n",
    "5. Repeat!\n",
    "\n",
    "When we provide enough trading data, this process allows us to fine-tune the perceptron's weights to predict as accurately as possible the locations of flowers.\n",
    "\n",
    "## Translating this algorithm into code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method for the perceptron class\n",
    "def train(self: Perceptron, inputs, answer):\n",
    "    # Finding what the perceptron predicts\n",
    "    guess = self.feedforward(inputs)\n",
    "    # Getting how wrong we were (and in what direction)\n",
    "    error = guess - answer\n",
    "\n",
    "    # Changing the weights to be closer to the expected value\n",
    "    for i in range(len(self.weights)):\n",
    "        self.weights[i] += self.c * error * inputs[i]\n",
    "\n",
    "\n",
    "# Assigning the method to the class\n",
    "Perceptron.train = train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But First!\n",
    "Before we actually run the code to train the perceptron, let's see how our perceptron with just random weights does with predicting things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"true\" distribution of the flowers\n",
    "# This could be anything, but we're just choosing it as something to compare our model to.\n",
    "def f(x):\n",
    "    return 2*x + 1\n",
    "\n",
    "# Function that compares the predictions to the actual output\n",
    "def evaluate():\n",
    "    xs = []\n",
    "    ys = []\n",
    "    outputs = []\n",
    "    # Guessing the color of a bunch of random flowers\n",
    "    for i in range(100):\n",
    "        x = random() * 50 - 25\n",
    "        y = random() * 50 - 25\n",
    "\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        outputs.append(perceptron.feedforward([x, y, 1]))\n",
    "\n",
    "\n",
    "    plt.scatter(xs, ys, c=outputs,cmap='viridis')\n",
    "\n",
    "    plt.plot([-25, 25], [f(-25), f(25)], color='green', marker='', linestyle='dotted')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty bad right? But now we'll train the perceptron on all of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually Training\n",
    "We already generated some data for our flowers, so lets just choose a few random points out of those to be our training sets: `flower_locations` (inputs) and `flower_colors` (outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much training data we want to use\n",
    "num_training_points = 1000\n",
    "\n",
    "# Select a random portion of the points we have.\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "# Generating a number of points to train with\n",
    "for i in range(num_training_points):\n",
    "    x = random() * 50 - 25\n",
    "    y = random() * 50 - 25    \n",
    "\n",
    "    # Getting the correct output based on our function\n",
    "    out = 1 if f(x) > y else -1\n",
    "\n",
    "    inputs.append((x,y,1))\n",
    "    outputs.append(out)\n",
    "\n",
    "# Training our perceptron\n",
    "for i in range(len(inputs)):\n",
    "    perceptron.train(inputs[i], outputs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing how we did\n",
    "Let's compare the predictions of our perceptron to the line y=2x + 1 that represents the actual distribution we're aiming for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function to display our results\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much closer right?\n",
    "\n",
    "If you want, you can even run the code block again to generate some more data, and you should see the predictions get even closer to the actual line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We looked at how neural networks work by seeking to model human neurons.\n",
    "\n",
    "We then created and trained our own perceptron to predict the color of flowers in a field.\n",
    "\n",
    "Although this model is extremely simple, by connecting perceptrons together it's possible to train neural networks to model and predict much more complex phenomena. Hopefully by walking through this example, you have a better sense of what is going on at the very core of one of the most important tools in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- https://natureofcode.com/book/chapter-10-neural-networks/\n",
    "- https://en.wikipedia.org/wiki/Machine_learning#Model_assessments\n",
    "- https://en.wikipedia.org/wiki/Artificial_neural_network\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
